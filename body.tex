\input{introduction}

\section{Survey Simulator Overview}
\label{section:simulator}
Probably need some reference to what survey scheduler was used / how it was set up for various runs, how the runs were performed, and what the input weather and telescope models were like. 

Earlier attempts at simulating LSST in \citet{Rothchild19} and \citet{Naghib19}. ?

\subsection{The Model Observatory}
Discuss kinematic model, seeing model, weather model. 

\subsection{The Scheduler}

The scheduler is designed to provide real-time decisions on where and how to observe. Because we expect there to be things like weather interruptions, we need a system that can recover quickly. Unlike other traditional telescope schedulers, we do not try to optimize a large number of observations in advance, but rather use a decision tree along with a modified Markov Decision Process. The scheduler behavior is set by a small number of free parameters that can be tuned.

Our baseline scheduler uses a three tier decision tree when deciding what observations to attempt. 

\subsubsection{Tier 1:  Deep Drilling Fields}

The first tier of the decision tree is to check if there are any deep drilling fields that should be executed. We typically have five DDFs in a simulation. 

For a DDF to be eligible to send a sequence to the observing queue, it must
\begin{itemize}
\item{Not currently be twilight}
\item{Have enough time to finish a sequence before twilight begins}
\item{Be in it's target hour angle range}
\item{The moon must be down}
\item{The DDF must not have exceeded it's limit of observations (typically $\sim$1\% of the total number of visits)}
\end{itemize}

If the DDF has not fallen behind, it will space sequences by at least 1.5 days. There is also a check to see if the DDF will be feasible and better observed later in the night, in which case no observations are requested.

If the above conditions are met, the DDF sends it's sequence of observations to the queue to be executed. There are currently no attempts at recovery if a sequence is interrupted. 

The spatial position of the DDF is dithered nightly up to 0.7 degrees.  The camera rotator is also varied nightly to be between -75 and 75 degrees with respect to the telescope. 

\begin{table}
\begin{tabular}{lcc}
\toprule
    Name &      RA &     Dec \\
    &          (Deg) &  (Deg) \\
    \hline
 ELAISS1 &   9.450 & -44.000 \\
 XMM-LSS &  35.708 &  -4.750 \\
   ECDFS &  53.125 & -28.100 \\
  COSMOS & 150.100 &   2.182 \\
    EDFS &  58.970 & -49.280 \\
    EDFS &  63.600 & -47.600 \\
    \hline
\end{tabular}
\caption{The location of the deep drilling fields used in our simulations.}\label{table:ddfs}
\end{table}



\subsubsection{Tier 2:  The Blobs}

If there are no DDFs requesting observations, the decision tree moves to the second tier. This tier is the survey workhorse, executing $\sim$80\% of the simulation visits.  This tier will only request observations if it is not currently twilight, and there is at least 30 minutes before twilight begins.

A modified Markov Decision Process (MDP) is used to decide what sky region and filter combination to observe given the current conditions and observation history. Briefly, the MDP balances the desire to observe areas 1) that are closest to the optimal possible in terms of 5-sigma depth, 2) which have fallen behind the specified desired survey footprint, 3) are near the current telescope pointing and 4) in the currently loaded filter to minimize filter changes.  In addition to these core components, the MDP includes a mask around zenith, a 30 degree mask around the moon, and small masks around the bright planets (Venus, Mars, Jupiter). The end product of the MDP is a reward function that ranks the desirability of every point in the sky. Because this tier does not execute in twilight, we assume the reward function is relatively stable on 40 minute timescales.

A sky area around the reward function maximum that will take $\sim$22 minutes to observe ($\sim$35 pointings) is then selected. If possible, the area is selected to be be contiguous.  The exact position of the telescope pointings are determined by the sky tessellation, which is randomly oriented for each night. The camera rotator angle (relative to the telescope) is also randomized between $\pm 80$\ degrees each night.

A traveling salesman algorithm is used to put the pointings in an order that minimizes the slew time. The list of pointings are then repeated, usually in a different filter, ensuring moving objects can be detected.  One of seven possible filter combinations is used: $u+g$, $u+r$, $g+r$, $r+i$, $i+z$, $z+y$, or $y+y$.  We use 30 second visits for the majority of simulations. The official baseline uses visits comprised of two 15 second snaps.  


\subsubsection{Tier 3:  Greedy}

If it is during morning or evening twilight, or close to morning twilight, the DDFs and Blob surveys will pass and the decision tree goes to the third and final tier, the greedy surveys. 

The greedy surveys use a similar Markov Decision Process as in Tier 2, but rather than selecting large areas of sky to observe, the survey selects a single pointing at a time.  No attempt is made to observe greedy scheduled observations in pairs.  Since this tier is primarily used in twilight time, it only schedules observations in the redder filters $r$, $i$, $z$, and $y$.  

As with the Blob tier, the sky tessellation orientation is randomized each night so the final survey is spatially dithered. 



\begin{figure}
\epsscale{0.3}
\plotone{plots/night_plots/baseline_nexp1_v1_6_Count_note_like_DD_and_night810_HEAL_SkyMap.pdf}
\plotone{plots/night_plots/baseline_nexp1_v1_6_Count_note_like_blob_and_night810_HEAL_SkyMap.pdf}
\plotone{plots/night_plots/baseline_nexp1_v1_6_Count_note_like_greedy_and_night810_HEAL_SkyMap.pdf}

\plotone{plots/night_plots/baseline_nexp1_v1_6_altAz_Count_note_like_DD_and_night810_HEAL_SkyMap.pdf}
\plotone{plots/night_plots/baseline_nexp1_v1_6_altAz_Count_note_like_blob_and_night810_HEAL_SkyMap.pdf}
\plotone{plots/night_plots/baseline_nexp1_v1_6_altAz_Count_note_like_greedy_and_night810_HEAL_SkyMap.pdf}
\epsscale{1}
\caption{Examples of how the three scheduler tiers execute during a single night. Left panels show how a DDF sequence was observed during the night. Middle panels show observations taken as part of blob pairs. Right panels show the greedy observations taken in twilight time.} \label{fig:examplenight}
\end{figure}

\subsection{Filter Mounting Schedule}

XXX--discuss how we decide which filters to have mounted at a given time.


\section{Basic Survey Requirements}
Basic survey strategy starting point and why - in more depth? Discuss metrics related to these requirements. 

Probably should show that all survey strategies evaluated do / need to meet these requirements (but maybe later?)

XXX--Relevant SRD requirements. 825 observations over 18,000 square degrees, fast revisits, and astrometry

XXX--general requirement to advance all 4 pillars of Rubin science

XXX--relevant requirement to publish a list of upcoming planned observations (1?2?) hours in advance. 

\section{Feedback from white papers and SAC} 
Broad outline of points to evaluate for survey strategy, and our approach in running the subsequent experiments (this should help make sense of what comes next)

Discuss basic types of SAC recommendations. 

\section{Overview of Metrics}
XXX--maybe a subset of the most important metrics? 
%\input{metrics}

\input{runs_v15}

\input{runs_v16}


\section{Individual Visit Length}
What to do - 1x30s vs. 2x15s? 1x30s much more efficient (show rough calculation of overhead) than 2x15s, but may have drawbacks due to cosmic ray rejection and potential to miss very rapid transients (or WD detection .. ref white paper). Subtle drawback that 2x15s gives the same "midpoint exposure time" across FOV, 1x30s does not. 

Show difference in 1x30s vs. 2x15s in whatever is our 'standard baseline' at this point. 

There has been thought of using a variety of exposure times if we use two snaps (e.g., 5s + 25s). Because there are not plans to release catalogs from individual snaps, it's not clear if this would enable much new science.

Show effect of 7\% loss in efficiency when attempting to combine minisurveys in various configurations (assume we will find some combinations possible with single exposure visits that are impossible with two snaps). 

Also possible to use variable exposure time depending on seeing and sky brightness conditions. Shorter exposures in good conditions keeps us from observing ``wasted" depth, letting us take longer exposures in poor conditions. This does introduce a host of new free parameters (an ideal target depth for each filter and minimum and maximum exposure times).  This would might require rewording the SRD to ensure, e.g., that 20s visits in good conditions count for the number of visit requirement.

Relevant metrics: total number of visits, number of visits per field/filter

\section{Intra-night Cadence}
What to do for visit sequence within a night? White paper support for multiple filters within a night (except TNOs maybe?). Potential drawbacks - less efficient (show effect on efficiency). This applies to WFD primarily, but we've applied to any survey that did not have their own specifications (so, everywhere). 

Extension of pairs to $u$ band and $y$ band (show effect). 

Relevant metrics: inter-night visit gaps and SN discovery, SSO discovery/characterization, transient and variable discovery (??), number of visits

\section{Wide-Fast-Deep Footprint}
What to do for WFD footprint? SRD not specific, DESC want low-extinction sky (and depth), but WFD is generally the area of sky that receives the most visits, so generally other science will also benefit from more visits to their relevant areas (particularly galactic plane .. for time-domain studies primarily, not depth)

Relevant metrics: area of sky with 825 visits (under particular restrictions, like total coadded depth and individual image seeing and dust extinction), number of galaxies, number of resolved galaxies, SSO discovery, transient and variable star discovery, astrometry in the galactic plane (?)


\section{Rolling cadence}
Motivation for a rolling cadence (more frequent visits in some years)

Different options for rolling and explanation of how implemented

Should really include discussion of recovery from bad weather years and simulation of same

Relevant metrics: Maintain astrometry requirements, SN discovery, SSO discovery and characterization,  Transient and variable discovery, uniformity of coadded depth / number of visits, 

\section{Northern minisurveys}
Add extension to cover Euclid/DESI with various numbers of visits

Observing NES 

Effect of adding or removing these minisurveys

Relevant metrics: SSO discovery and characterization (particularly active asteroids), depth and number of visits through remainder of North

\section{Southern minisurveys}
Add extension over south celestial pole, LMC/SMC with various numbers of visits

Effect of adding or removing these minisurveys

Relevant metrics: number of visits and coadded depth over SCP, discovery of variables in LMC/SMC (see Olsen white paper for metrics?)

\section{Low Galactic Latitudes}
Discussion of definitions from SAC and recommendations for visits

Effect of adding or removing these minisurveys

Relevant metrics: number of visits, astrometry in bulge, discovery of variables/transients/microlensing in bulge (?)

\section{Twilight Observing}
Discuss need for twilight observing to meet SRD goals (weather, total amount of time available)

Add NEO twilight survey, add DCR white paper (season extension visits?)

Effect of adding or removing these minisurveys

Relevant metrics: NEO discovery, number of visits and coadded depth (and uniformity) in WFD, measurement of DCR, season length

\section{Deep Drilling Fields}
Discuss purpose and how these are scheduled (very different from other fields)

Discuss potential cadences (AGN/ DESC) and how these differ, and our combination of the two

Discuss timing issues with oversubscription (and how much of a problem this could be, what if worse weather?) -- include location of fifth DD field

Effect of adding or removing these minisurveys

Relevant metrics: number of visits and coadded depth for DD, SN detection in DDFs, AGN detection in DDFs
*[solar system minisurvey DDF?]

\section{ToO modes}
Discuss impact of ToO, and how we could implement ToOs in scheduler (various modes: straight to queue by hand or set up known program and supply trigger, etc. -- that we're evaluating the second?)

Any ToO survey should also take into account that chip and raft gaps mean full sky coverage will require multiple images with spatial dithering.

Discuss how we can have a low coverage region to the north to maintain templates for all possible ToOs, or we could decide ot only search for ToOs that are likely to be in the WFD area.

Relevant metrics: frequency of achieving ToO observations, number of visits and coadded depth in other surveys (WFD or other minisurveys that may be in particular contention)


\section{Further Optimizing}
Somewhere in here we probably ought to talk about optimizing the parameters for each run, and doing bigger sweeps across parameter space. That would easily expand each of the above options by many factors.

XXX--need to optimize basis function weights for both the blobs and the greedy algo.

XXX--can also discuss pre-scheduling the DDF sequences here.

\section{Conclusions}
Hopefully here we pare down the evaluation of 100s of runs (like promised) to a set of between 10 to 20 (if this is possible, after combining along different axes). 
The results should come with some basic comments about what's particularly good or bad in each of these areas and how we arrived at these general options. 

\input{questions}


% Make sure lsst-texmf/bin/generateAcronyms.py is in your path
\section{Acronyms} \label{sec:acronyms}
\input{acronyms.tex}
